{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean version available at:  \n",
    "- https://github.com/Personalization-Technologies-Lab/RecSys-Course-HSE-Fall23/tree/main/Seminar6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing packages:\n",
    "```\n",
    "# polara\n",
    "pip install --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara\n",
    "\n",
    "# ipypb\n",
    "pip install ipypb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from pprint import pprint\n",
    "from ipypb import track\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_stackexchange\n",
    "\n",
    "from polara.evaluation.pipelines import random_grid\n",
    "from polara.lib.earlystopping import early_stopping_callback\n",
    "from polara.tools.display import print_frames\n",
    "\n",
    "from evaluation import topn_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is adapted from official `LightFM`'s documentation for a cold-start scenario:  \n",
    "https://making.lyst.com/lightfm/docs/examples/hybrid_crossvalidated.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the StackExchange data dump. The dataset consists of users and questions they answered.\n",
    "\n",
    "**Task**:  \n",
    "Find users that are most qualified for answering new questions.\n",
    "\n",
    "Your recommendation algorithm must tailor matching between users and questions based on user expertise. You will need to use hybrid approach that utilizes side information about items. The dataset contains question labels in the form of user-assigned `tags`. Hence, even though questions will be \"cold\" (i.e., unanswered), you can still find the best match between experts and questions based on their answering history and tags used in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_stackexchange(\n",
    "    'crossvalidated',\n",
    "    test_set_fraction=0.1,\n",
    "    indicator_features=False,\n",
    "    tag_features=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sparse matrices back into dataframes for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ['users', 'questions']\n",
    "training_data = pd.DataFrame(dict(zip(entities, data['train'].nonzero())))\n",
    "test_data = pd.DataFrame(dict(zip(entities, data['test'].nonzero())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['item_feature_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['item_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tags = (\n",
    "    pd.DataFrame(dict(zip(['questions', 'tagid'], data['item_features'].nonzero())))\n",
    "    .assign(tags = lambda x: x['tagid'].map(data['item_feature_labels'].__getitem__))\n",
    "    .groupby('questions')\n",
    "    ['tags'].apply(list)\n",
    "    .to_frame('tags')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames([\n",
    "    training_data.head(), # data for training and validation\n",
    "    test_data.head(), # data for testing\n",
    "    item_tags.head() # item features data\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset already provides splitting. But an additional step is still required:  \n",
    "- splitting into validation and actual test parts.\n",
    "\n",
    "To simplify evaluation, only a single true expert will be withheld from each \"cold\" question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=107)\n",
    "\n",
    "final_test = (\n",
    "    test_data\n",
    "    .sample(frac=1, random_state=rng) # shuffle data\n",
    "    .drop_duplicates(subset=['questions'])\n",
    "    .sample(frac=0.55, random_state=rng) # make test and validation sizes more balanced\n",
    "    .sort_values('questions')\n",
    ")\n",
    "\n",
    "validation = (\n",
    "    test_data\n",
    "    .drop(final_test.index)\n",
    "    .sample(frac=1, random_state=rng)\n",
    "    .drop_duplicates(subset=['questions'])\n",
    "    .sort_values('questions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames([validation.head(), final_test.head()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lfm_model(config, data, data_description, early_stop_config=None, iterator=None):\n",
    "    \"\"\"\n",
    "    Builds a LightFM model using the given configuration, data and data description.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        A dictionary containing the configuration for the model. It must contain the following keys:\n",
    "        'no_components', 'max_sampled', 'loss', 'learning_schedule', 'user_alpha' and 'item_alpha'.\n",
    "    data : sparse matrix of interactions in COO format of shape (n_users, n_items)\n",
    "        The training data.\n",
    "    data_description : dict\n",
    "        A dictionary containing information about the data. It must contain the following keys:\n",
    "        'interactions', 'user_features' and 'item_features'.\n",
    "    early_stop_config : dict, optional (default=None)\n",
    "        A dictionary containing early stopping configuration. If not provided, default values will be used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : LightFM object The trained LightFM model.\n",
    "    \"\"\"\n",
    "    # the model\n",
    "    model = LightFM(\n",
    "        ...\n",
    "    )\n",
    "    # early stoppping configuration\n",
    "    es_config = check_early_stop_config(early_stop_config)\n",
    "\n",
    "    # training\n",
    "    if iterator is None:\n",
    "        iterator = lambda x: x\n",
    "    for epoch in iterator(range(config['max_epochs'])):\n",
    "        try:\n",
    "            train_lfm_epoch(epoch, model, data, data_description, es_config)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "def check_early_stop_config(early_stop_config):\n",
    "    \"\"\"\n",
    "    Validates the early stop configuration and returns a config dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    early_stop_config : dict, optional\n",
    "        Dictionary containing the early stop configuration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    es_dict : dict\n",
    "        Dictionary containing the early stop configuration, or a dictionary\n",
    "        with 'stop_early' set to False if no valid configuration is provided.\n",
    "    \"\"\"\n",
    "    if early_stop_config is None:\n",
    "        early_stop_config = {}\n",
    "    try:\n",
    "        es_dict = {\n",
    "            'early_stopper': early_stop_config['evaluation_callback'],\n",
    "            'callback_interval': early_stop_config['callback_interval'],\n",
    "            'holdout': early_stop_config['holdout'],\n",
    "            'stop_early': True\n",
    "        }\n",
    "    except KeyError: # config is invalid, doesn't contain required keys\n",
    "        es_dict = {'stop_early': False} # disable early stopping\n",
    "    return es_dict\n",
    "\n",
    "\n",
    "def train_lfm_epoch(\n",
    "    epoch, model, train, data_description, es_config,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a LightFM model for a single epoch. Optionally validate the model\n",
    "    and raise StopIteration if the early stopping condition is met.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : int\n",
    "        The current epoch number.\n",
    "    model : LightFM object\n",
    "        The LightFM model to be trained.\n",
    "    train : scipy.sparse matrix\n",
    "        The training data matrix in COO format.\n",
    "    data_description : dict\n",
    "        A dictionary containing the user and item feature matrices.\n",
    "    es_config : dict\n",
    "        A dictionary containing the early stopping configuration parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    StopIteration: If the early stopping condition is met.\n",
    "    \"\"\"\n",
    "    model.fit_partial(\n",
    "        train,\n",
    "        user_features = ...,\n",
    "        item_features = ...,\n",
    "        epochs = 1\n",
    "    )\n",
    "    if not es_config['stop_early']:\n",
    "        return\n",
    "\n",
    "    metrics_check_interval = es_config['callback_interval']\n",
    "    if (epoch+1) % metrics_check_interval == 0:\n",
    "        # evaluate model and raise StopIteration if early stopping condition is met\n",
    "        early_stopper_call = es_config['early_stopper']\n",
    "        early_stopper_call(epoch, model, es_config['holdout'], data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightfm_scoring(model, data, data_description):\n",
    "    \"\"\"\n",
    "    A standard scoring function adopted for use with LightFM in the item cold-start settings.\n",
    "    It returns a 2D item-user array (i.e., a transposed matrix of interactions) corresponding\n",
    "    to the predicted scores of user relevance to cold items.\n",
    "    \"\"\"\n",
    "    dtype = 'i4'\n",
    "    all_users = ...\n",
    "    test_items = ...\n",
    "    item_index, user_index = ...\n",
    "\n",
    "    lfm_scores = model.predict(\n",
    "        ...\n",
    "    )\n",
    "    scores = lfm_scores.reshape(len(test_items), len(all_users), order='F')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is based on the `polara`'s `early_stopping_callback` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coldstart_evaluate(recommended_users, holdout, data_description, topn=10):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a recommender system for item cold-start scenario.\n",
    "    It takes in `recommended_users` - a 2D array of recommended users for each cold item in the holdout.\n",
    "    It returns a dictionary containing with evaluation metrics.\n",
    "    \"\"\"\n",
    "    userid = data_description['users']\n",
    "    holdout_users = holdout[userid].values\n",
    "    assert recommended_users.shape[0] == len(holdout_users)\n",
    "\n",
    "    hits_mask = recommended_users[:, :topn] == holdout_users.reshape(-1, 1)\n",
    "    # HR calculation\n",
    "    hr = np.mean(hits_mask.any(axis=1))\n",
    "    # MRR calculation\n",
    "    n_test_items = recommended_users.shape[0]\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = np.sum(1 / hit_rank) / n_test_items\n",
    "    return {'hr': hr, 'mrr': mrr}\n",
    "\n",
    "\n",
    "def lfm_evaluator(model, holdout, data_description, target_metric='hr'):\n",
    "    \"\"\"\n",
    "    Helper function to run within an evaluation callback.\n",
    "\n",
    "    Intended usage:\n",
    "    - in the early stopping setting for tuning based on a `target_metric`.\n",
    "    \"\"\"\n",
    "    lfm_scores = lightfm_scoring(model, None, data_description)\n",
    "    lfm_recs = topn_recommendations(lfm_scores)\n",
    "    metrics = coldstart_evaluate(lfm_recs, holdout, data_description)\n",
    "    return metrics[target_metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_config = dict(\n",
    "    no_components = 30,\n",
    "    loss = 'warp',\n",
    "    max_sampled = 3,\n",
    "    max_epochs = 100,\n",
    "    learning_schedule = 'adagrad',\n",
    "    user_alpha = 1e-3,\n",
    "    item_alpha = 1e-3,\n",
    ")\n",
    "\n",
    "try_early_stop = early_stopping_callback(\n",
    "    lfm_evaluator, max_fails=3, verbose=True\n",
    ")\n",
    "\n",
    "early_stop_config = dict(\n",
    "    evaluation_callback = try_early_stop,\n",
    "    callback_interval = 10, # num of epochs between consequent evaluations\n",
    "    holdout = validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = 'users',\n",
    "    items = 'questions',\n",
    "    n_users = data['train'].shape[0],\n",
    "    cold_items = validation['questions'].values,\n",
    "    user_features = data.get('user_features'),\n",
    "    item_features = data.get('item_features'),\n",
    ")\n",
    "data_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_params = build_lfm_model(\n",
    "    lfm_config,\n",
    "    data['train'],\n",
    "    data_description,\n",
    "    early_stop_config = early_stop_config,\n",
    "    iterator = track\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning uses the `polara`'s `random_grid` function for sampling random configurations from a defined hyper-parameters space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyper-parameters space\n",
    "lfm_params_grid = dict(\n",
    "    no_components = [8, 12, 16, 24, 32, 48, 64],\n",
    "    loss = ['warp'],\n",
    "    max_sampled = [3, 10, 30, 100],\n",
    "    max_epochs = [100],\n",
    "    learning_schedule = ['adagrad'],\n",
    "    user_alpha = [1e-5],\n",
    "    item_alpha = [1e-5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the random grid\n",
    "param_grid, param_names = random_grid(lfm_params_grid, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_config = dict(\n",
    "    callback_interval = 10, # break between consequent evaluation in epochs\n",
    "    holdout = validation,\n",
    ")\n",
    "\n",
    "lfm_results = {}\n",
    "\n",
    "for grid_params in track(param_grid):\n",
    "    lfm_config = dict(zip(param_names, grid_params))\n",
    "    early_stop_config['evaluation_callback'] = es_call = early_stopping_callback(\n",
    "        lfm_evaluator, max_fails=3, verbose=False\n",
    "    )\n",
    "    lfm_params = build_lfm_model(\n",
    "        lfm_config,\n",
    "        data['train'],\n",
    "        data_description,\n",
    "        early_stop_config = early_stop_config,\n",
    "    )\n",
    "    num_epochs = es_call.iter + 1 # store optimal number of epochs\n",
    "    lfm_results[grid_params+(num_epochs,)] = es_call.target # store optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lfm_config = dict(\n",
    "    zip(\n",
    "        param_names + ('max_epochs',),\n",
    "        pd.Series(lfm_results).idxmax()\n",
    "    )\n",
    ")\n",
    "pprint(optimal_lfm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Final training is performed on a joint dataset consisting of previous train and validation datasets.\n",
    "- Evaluation is performed based on a final holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_from_observations(data, data_description, dtype='f4'):\n",
    "    useridx = data[data_description['users']]\n",
    "    itemidx = data[data_description['items']]\n",
    "    values = np.ones(data.shape[0])\n",
    "    return csr_matrix((values, (useridx, itemidx)), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = pd.concat(\n",
    "    [...],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "train_matrix = matrix_from_observations(\n",
    "    final_train, data_description, dtype=data['train'].dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_params = build_lfm_model(\n",
    "    optimal_lfm_config,\n",
    "    train_matrix,\n",
    "    data_description,\n",
    "    early_stop_config = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description['cold_items'] = final_test[data_description['items']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_scores = lightfm_scoring(lfm_params, None, data_description)\n",
    "lfm_recs = topn_recommendations(lfm_scores)\n",
    "metrics = coldstart_evaluate(lfm_recs, final_test, data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3e52df71fc60bf6067bf42b1ea293f87907f7f853acd0373bd6f3f7e1a122c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
